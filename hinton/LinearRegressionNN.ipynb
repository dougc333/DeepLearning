{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Linear Regression </h4>\n",
    "\n",
    "Brief derivation of OLS meethod of Linear Regression. \n",
    "There are many alternate derivations \n",
    "Assume a system of linear equations or assume a set of points with the goal of \n",
    "drawing a line through them. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"linreg.png\" height=\"300\", width=\"300\">\n",
    "\n",
    "For a set of input points$x_i, y_i$ define a least squares distance measure: \n",
    "    \n",
    "$error = (y_i-x_i)^2$\n",
    "\n",
    "Take the derivative to minimize this error and set it to 0 to calculate the coefficients:\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_examples1 = np.array([[0.871429,0.624585,1],[ -0.020000,-0.923588,1],\n",
    "                         [0.362857,-0.318937,1],[0.888571,-0.870432,1]])\n",
    "\n",
    "neg_examples1 = np.array([[-0.80857,0.83721,1],[0.35714,0.85050,1],\n",
    "                         [-0.75143,-0.73090,1],[-0.30000,0.12625,1]\n",
    "                        ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The simplest neuron is a linear neuron used for performing linear separation. \n",
    "\n",
    "\n",
    "<img src=\"linearnn1.png\">\n",
    "\n",
    "We define the operation of the neuron as multiplying the input with a weight: $w_i$. \n",
    "Define the loas function as the expected output minus the actual squared. $loss = \\frac{1}{2}\\sum\\limits_{i=1}^n(t_n-y_n)^2$. \n",
    "We are going to take the sum of all the errors for all the sample points and update the weights. This is batch training. \n",
    "If we process 1 sample at at time this is online training. To find the values of weights we use gradient descent. \n",
    "\n",
    "\n",
    "We want to minimize loss as a function of the weights. Using the chain rule we can expand the partial deriviative WRT w\n",
    "as follows: \n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{\\partial y_i}{\\partial w_i} \\cdot \\frac{\\partial loss}{\\partial y_i}$\n",
    "\n",
    "$\\frac{\\partial loss}{\\partial w_i} = \\frac{1}{2} \\cdot (2)(t_i-y_i)(-1) = -(t_i-y_i)$\n",
    "\n",
    "$y_i = \\sum\\limits_{i=1}^N w_ix_i$ so $\\frac{\\partial y_i}{\\partial w_i} = \\sum\\limits_{i=1}^N x_i$\n",
    "\n",
    "$w_{i+1} = w_i+\\delta \\cdot w_i$\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\frac{\\partial loss}{\\partial w_i} $\n",
    "where $\\epsilon$ is the learning rate\n",
    "\n",
    "$\\delta w_i = \\epsilon \\cdot \\sum\\limits_{i=1}^N x_i(t_i-y_i)$\n",
    "\n",
    "Set the initial weights to a random value with variance=1 and compute the delta w for all the training samples and\n",
    "keep iterating until the weight values converge. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n",
      "iris.feature_names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "iris.data: [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 7.6  3.   6.6  2.1]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.7  2.5  5.8  1.8]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n",
      "iris.target_names: ['setosa' 'versicolor' 'virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "x: [ 0.2  0.2  0.2  0.2  0.2  0.4  0.3  0.2  0.2  0.1  0.2  0.2  0.1  0.1  0.2\n",
      "  0.4  0.4  0.3  0.3  0.3  0.2  0.4  0.2  0.5  0.2  0.2  0.4  0.2  0.2  0.2\n",
      "  0.2  0.4  0.1  0.2  0.1  0.2  0.2  0.1  0.2  0.2  0.3  0.3  0.2  0.6  0.4\n",
      "  0.3  0.2  0.2  0.2  0.2  1.4  1.5  1.5  1.3  1.5  1.3  1.6  1.   1.3  1.4\n",
      "  1.   1.5  1.   1.4  1.3  1.4  1.5  1.   1.5  1.1  1.8  1.3  1.5  1.2  1.3\n",
      "  1.4  1.4  1.7  1.5  1.   1.1  1.   1.2  1.6  1.5  1.6  1.5  1.3  1.3  1.3\n",
      "  1.2  1.4  1.2  1.   1.3  1.2  1.3  1.3  1.1  1.3  2.5  1.9  2.1  1.8  2.2\n",
      "  2.1  1.7  1.8  1.8  2.5  2.   1.9  2.1  2.   2.4  2.3  1.8  2.2  2.3  1.5\n",
      "  2.3  2.   2.   1.8  2.1  1.8  1.8  1.8  2.1  1.6  1.9  2.   2.2  1.5  1.4\n",
      "  2.3  2.4  1.8  1.8  2.1  2.4  2.3  1.9  2.3  2.5  2.3  1.9  2.   2.3  1.8]\n",
      "y: [ 5.1  4.9  4.7  4.6  5.   5.4  4.6  5.   4.4  4.9  5.4  4.8  4.8  4.3  5.8\n",
      "  5.7  5.4  5.1  5.7  5.1  5.4  5.1  4.6  5.1  4.8  5.   5.   5.2  5.2  4.7\n",
      "  4.8  5.4  5.2  5.5  4.9  5.   5.5  4.9  4.4  5.1  5.   4.5  4.4  5.   5.1\n",
      "  4.8  5.1  4.6  5.3  5.   7.   6.4  6.9  5.5  6.5  5.7  6.3  4.9  6.6  5.2\n",
      "  5.   5.9  6.   6.1  5.6  6.7  5.6  5.8  6.2  5.6  5.9  6.1  6.3  6.1  6.4\n",
      "  6.6  6.8  6.7  6.   5.7  5.5  5.5  5.8  6.   5.4  6.   6.7  6.3  5.6  5.5\n",
      "  5.5  6.1  5.8  5.   5.6  5.7  5.7  6.2  5.1  5.7  6.3  5.8  7.1  6.3  6.5\n",
      "  7.6  4.9  7.3  6.7  7.2  6.5  6.4  6.8  5.7  5.8  6.4  6.5  7.7  7.7  6.\n",
      "  6.9  5.6  7.7  6.3  6.7  7.2  6.2  6.1  6.4  7.2  7.4  7.9  6.4  6.3  6.1\n",
      "  7.7  6.3  6.4  6.   6.9  6.7  6.9  5.8  6.8  6.7  6.7  6.3  6.5  6.2  5.9]\n",
      "x shape: (150,)\n",
      "y shape: (150,)\n",
      "global_step: <tf.Variable 'global_step_42:0' shape=() dtype=int32_ref>\n",
      "learning_rate: Tensor(\"learning_rate_41:0\", shape=(), dtype=float32)\n",
      "epochs: 0  epoch_loss: 2.86764  m: [[ 2.27027941]]  b: [[ 1.70067668]]\n",
      "epochs: 1  epoch_loss: 2.18066  m: [[ 2.44874883]]  b: [[ 1.95034432]]\n",
      "epochs: 2  epoch_loss: 1.97736  m: [[ 2.47789693]]  b: [[ 2.10258174]]\n",
      "epochs: 3  epoch_loss: 1.84801  m: [[ 2.45299387]]  b: [[ 2.21661448]]\n",
      "epochs: 4  epoch_loss: 1.7358  m: [[ 2.40930724]]  b: [[ 2.31446815]]\n",
      "epochs: 5  epoch_loss: 1.63245  m: [[ 2.35986543]]  b: [[ 2.40437746]]\n",
      "epochs: 6  epoch_loss: 1.53637  m: [[ 2.30945516]]  b: [[ 2.48946309]]\n",
      "epochs: 7  epoch_loss: 1.44692  m: [[ 2.25980997]]  b: [[ 2.57094741]]\n",
      "epochs: 8  epoch_loss: 1.36363  m: [[ 2.21153307]]  b: [[ 2.64934707]]\n",
      "epochs: 9  epoch_loss: 1.28608  m: [[ 2.16480994]]  b: [[ 2.72491503]]\n",
      "epochs: 10  epoch_loss: 1.21387  m: [[ 2.11967254]]  b: [[ 2.79780388]]\n",
      "epochs: 11  epoch_loss: 1.14662  m: [[ 2.07609749]]  b: [[ 2.86812758]]\n",
      "epochs: 12  epoch_loss: 1.08401  m: [[ 2.03404212]]  b: [[ 2.93598342]]\n",
      "epochs: 13  epoch_loss: 1.02571  m: [[ 1.99345744]]  b: [[ 3.00146031]]\n",
      "epochs: 14  epoch_loss: 0.971415  m: [[ 1.95429373]]  b: [[ 3.06464291]]\n",
      "epochs: 15  epoch_loss: 0.920862  m: [[ 1.91650152]]  b: [[ 3.12561154]]\n",
      "epochs: 16  epoch_loss: 0.873789  m: [[ 1.88003325]]  b: [[ 3.18444419]]\n",
      "epochs: 17  epoch_loss: 0.829957  m: [[ 1.84484255]]  b: [[ 3.24121594]]\n",
      "epochs: 18  epoch_loss: 0.789142  m: [[ 1.81088459]]  b: [[ 3.29599857]]\n",
      "epochs: 19  epoch_loss: 0.751136  m: [[ 1.77811635]]  b: [[ 3.34886217]]\n",
      "epochs: 20  epoch_loss: 0.715747  m: [[ 1.74649596]]  b: [[ 3.39987373]]\n",
      "epochs: 21  epoch_loss: 0.682794  m: [[ 1.71598339]]  b: [[ 3.44909811]]\n",
      "epochs: 22  epoch_loss: 0.652109  m: [[ 1.68653977]]  b: [[ 3.49659801]]\n",
      "epochs: 23  epoch_loss: 0.623536  m: [[ 1.65812767]]  b: [[ 3.54243398]]\n",
      "epochs: 24  epoch_loss: 0.596931  m: [[ 1.63071096]]  b: [[ 3.5866642]]\n",
      "epochs: 25  epoch_loss: 0.572157  m: [[ 1.60425472]]  b: [[ 3.6293447]]\n",
      "epochs: 26  epoch_loss: 0.549088  m: [[ 1.57872534]]  b: [[ 3.67053008]]\n",
      "epochs: 27  epoch_loss: 0.527608  m: [[ 1.55409026]]  b: [[ 3.71027255]]\n",
      "epochs: 28  epoch_loss: 0.507606  m: [[ 1.53031826]]  b: [[ 3.74862289]]\n",
      "epochs: 29  epoch_loss: 0.488981  m: [[ 1.50737906]]  b: [[ 3.78562951]]\n",
      "epochs: 30  epoch_loss: 0.471638  m: [[ 1.48524356]]  b: [[ 3.82133985]]\n",
      "epochs: 31  epoch_loss: 0.455489  m: [[ 1.46388352]]  b: [[ 3.85579896]]\n",
      "epochs: 32  epoch_loss: 0.440452  m: [[ 1.44327176]]  b: [[ 3.88905096]]\n",
      "epochs: 33  epoch_loss: 0.42645  m: [[ 1.42338204]]  b: [[ 3.92113805]]\n",
      "epochs: 34  epoch_loss: 0.413411  m: [[ 1.40418911]]  b: [[ 3.95210099]]\n",
      "epochs: 35  epoch_loss: 0.401271  m: [[ 1.38566864]]  b: [[ 3.98197937]]\n",
      "epochs: 36  epoch_loss: 0.389966  m: [[ 1.36779702]]  b: [[ 4.01081085]]\n",
      "epochs: 37  epoch_loss: 0.379439  m: [[ 1.35055137]]  b: [[ 4.03863239]]\n",
      "epochs: 38  epoch_loss: 0.369637  m: [[ 1.33390999]]  b: [[ 4.06547928]]\n",
      "epochs: 39  epoch_loss: 0.36051  m: [[ 1.31785154]]  b: [[ 4.09138536]]\n",
      "epochs: 40  epoch_loss: 0.35201  m: [[ 1.30235577]]  b: [[ 4.11638403]]\n",
      "epochs: 41  epoch_loss: 0.344097  m: [[ 1.28740287]]  b: [[ 4.14050674]]\n",
      "epochs: 42  epoch_loss: 0.336727  m: [[ 1.27297378]]  b: [[ 4.1637845]]\n",
      "epochs: 43  epoch_loss: 0.329866  m: [[ 1.25905013]]  b: [[ 4.18624687]]\n",
      "epochs: 44  epoch_loss: 0.323476  m: [[ 1.24561429]]  b: [[ 4.20792246]]\n",
      "epochs: 45  epoch_loss: 0.317526  m: [[ 1.23264909]]  b: [[ 4.22883844]]\n",
      "epochs: 46  epoch_loss: 0.311986  m: [[ 1.22013819]]  b: [[ 4.24902153]]\n",
      "epochs: 47  epoch_loss: 0.306828  m: [[ 1.20806563]]  b: [[ 4.26849794]]\n",
      "epochs: 48  epoch_loss: 0.302024  m: [[ 1.1964159]]  b: [[ 4.287292]]\n",
      "epochs: 49  epoch_loss: 0.297551  m: [[ 1.18517423]]  b: [[ 4.30542755]]\n",
      "epochs: 50  epoch_loss: 0.293386  m: [[ 1.17432642]]  b: [[ 4.32292747]]\n",
      "epochs: 51  epoch_loss: 0.289508  m: [[ 1.16385877]]  b: [[ 4.33981466]]\n",
      "epochs: 52  epoch_loss: 0.285896  m: [[ 1.15375781]]  b: [[ 4.3561101]]\n",
      "epochs: 53  epoch_loss: 0.282533  m: [[ 1.14401066]]  b: [[ 4.37183475]]\n",
      "epochs: 54  epoch_loss: 0.279402  m: [[ 1.13460493]]  b: [[ 4.38700819]]\n",
      "epochs: 55  epoch_loss: 0.276487  m: [[ 1.12552881]]  b: [[ 4.40165043]]\n",
      "epochs: 56  epoch_loss: 0.273772  m: [[ 1.11677063]]  b: [[ 4.41577959]]\n",
      "epochs: 57  epoch_loss: 0.271243  m: [[ 1.10831928]]  b: [[ 4.4294138]]\n",
      "epochs: 58  epoch_loss: 0.268889  m: [[ 1.10016406]]  b: [[ 4.44257021]]\n",
      "epochs: 59  epoch_loss: 0.266697  m: [[ 1.09229445]]  b: [[ 4.455266]]\n",
      "epochs: 60  epoch_loss: 0.264656  m: [[ 1.08470058]]  b: [[ 4.4675169]]\n",
      "epochs: 61  epoch_loss: 0.262756  m: [[ 1.07737267]]  b: [[ 4.47933865]]\n",
      "epochs: 62  epoch_loss: 0.260986  m: [[ 1.07030153]]  b: [[ 4.49074602]]\n",
      "epochs: 63  epoch_loss: 0.259338  m: [[ 1.06347811]]  b: [[ 4.50175381]]\n",
      "epochs: 64  epoch_loss: 0.257804  m: [[ 1.05689383]]  b: [[ 4.51237583]]\n",
      "epochs: 65  epoch_loss: 0.256375  m: [[ 1.05054021]]  b: [[ 4.52262592]]\n",
      "epochs: 66  epoch_loss: 0.255044  m: [[ 1.04440916]]  b: [[ 4.53251696]]\n",
      "epochs: 67  epoch_loss: 0.253805  m: [[ 1.03849292]]  b: [[ 4.54206133]]\n",
      "epochs: 68  epoch_loss: 0.252652  m: [[ 1.03278387]]  b: [[ 4.55127144]]\n",
      "epochs: 69  epoch_loss: 0.251577  m: [[ 1.02727485]]  b: [[ 4.56015873]]\n",
      "epochs: 70  epoch_loss: 0.250577  m: [[ 1.02195895]]  b: [[ 4.56873512]]\n",
      "epochs: 71  epoch_loss: 0.249646  m: [[ 1.01682913]]  b: [[ 4.57701063]]\n",
      "epochs: 72  epoch_loss: 0.248779  m: [[ 1.01187909]]  b: [[ 4.58499622]]\n",
      "epochs: 73  epoch_loss: 0.247971  m: [[ 1.00710237]]  b: [[ 4.59270239]]\n",
      "epochs: 74  epoch_loss: 0.247219  m: [[ 1.00249302]]  b: [[ 4.60013819]]\n",
      "epochs: 75  epoch_loss: 0.246519  m: [[ 0.99804521]]  b: [[ 4.60731363]]\n",
      "epochs: 76  epoch_loss: 0.245867  m: [[ 0.99375319]]  b: [[ 4.61423779]]\n",
      "epochs: 77  epoch_loss: 0.24526  m: [[ 0.98961157]]  b: [[ 4.62091923]]\n",
      "epochs: 78  epoch_loss: 0.244694  m: [[ 0.98561502]]  b: [[ 4.62736654]]\n",
      "epochs: 79  epoch_loss: 0.244168  m: [[ 0.98175853]]  b: [[ 4.63358831]]\n",
      "epochs: 80  epoch_loss: 0.243678  m: [[ 0.97803706]]  b: [[ 4.63959217]]\n",
      "epochs: 81  epoch_loss: 0.243221  m: [[ 0.97444594]]  b: [[ 4.64538527]]\n",
      "epochs: 82  epoch_loss: 0.242796  m: [[ 0.9709807]]  b: [[ 4.6509757]]\n",
      "epochs: 83  epoch_loss: 0.2424  m: [[ 0.96763682]]  b: [[ 4.65637016]]\n",
      "epochs: 84  epoch_loss: 0.242032  m: [[ 0.96441007]]  b: [[ 4.66157579]]\n",
      "epochs: 85  epoch_loss: 0.241689  m: [[ 0.96129638]]  b: [[ 4.6665988]]\n",
      "epochs: 86  epoch_loss: 0.241369  m: [[ 0.95829183]]  b: [[ 4.67144585]]\n",
      "epochs: 87  epoch_loss: 0.241072  m: [[ 0.95539254]]  b: [[ 4.67612314]]\n",
      "epochs: 88  epoch_loss: 0.240795  m: [[ 0.95259482]]  b: [[ 4.68063688]]\n",
      "epochs: 89  epoch_loss: 0.240537  m: [[ 0.94989502]]  b: [[ 4.68499231]]\n",
      "epochs: 90  epoch_loss: 0.240296  m: [[ 0.94728982]]  b: [[ 4.68919516]]\n",
      "epochs: 91  epoch_loss: 0.240073  m: [[ 0.94477588]]  b: [[ 4.69325066]]\n",
      "epochs: 92  epoch_loss: 0.239864  m: [[ 0.94235003]]  b: [[ 4.69716406]]\n",
      "epochs: 93  epoch_loss: 0.23967  m: [[ 0.94000918]]  b: [[ 4.70094061]]\n",
      "epochs: 94  epoch_loss: 0.23949  m: [[ 0.93775034]]  b: [[ 4.7045846]]\n",
      "epochs: 95  epoch_loss: 0.239322  m: [[ 0.93557066]]  b: [[ 4.70810127]]\n",
      "epochs: 96  epoch_loss: 0.239165  m: [[ 0.93346721]]  b: [[ 4.71149445]]\n",
      "epochs: 97  epoch_loss: 0.239019  m: [[ 0.93143755]]  b: [[ 4.71476889]]\n",
      "epochs: 98  epoch_loss: 0.238883  m: [[ 0.92947894]]  b: [[ 4.71792841]]\n",
      "epochs: 99  epoch_loss: 0.238757  m: [[ 0.927589]]  b: [[ 4.72097731]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGQghbuISAb6//naXKkcA1xRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117cc7d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print (type(iris))\n",
    "print( \"iris.feature_names:\",iris.feature_names)\n",
    "print (\"iris.data:\",iris.data)\n",
    "print(\"iris.target_names:\", iris.target_names)\n",
    "print (iris.target)\n",
    "\n",
    "#pick first and last columns to do regression on\n",
    "x = np.array([x[3] for x in iris.data])\n",
    "y = np.array([y[0] for y in iris.data])\n",
    "\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "\n",
    "print(\"x shape:\",x.shape)\n",
    "print(\"y shape:\",y.shape)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False,name=\"global_step\")\n",
    "\n",
    "#model\n",
    "x_ = tf.placeholder(tf.float32, shape=[None,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "m = tf.Variable(tf.zeros(shape=[1,1]))\n",
    "b = tf.Variable(tf.zeros(shape=[1,1]))\n",
    "\n",
    "output = tf.add(tf.matmul(x_,m),b)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_ - output))\n",
    "\n",
    "print(\"global_step:\", global_step)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=0.1, global_step=global_step,\n",
    "    decay_steps=100, decay_rate=0.001,name=\"learning_rate\")\n",
    "print(\"learning_rate:\", learning_rate)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "rand_index = np.random.choice(len(x), size=len(x))\n",
    "#print(\"rand_index:\", rand_index)\n",
    "rand_x = np.transpose([x[rand_index]])\n",
    "rand_y = np.transpose([y[rand_index]])\n",
    "x_transpose = np.transpose(x)\n",
    "y_transpose = np.transpose(y)\n",
    "#print(\"rand_x:\",rand_x)\n",
    "#print(\"rand_y:\",rand_y)\n",
    "sess.run(init)\n",
    "sess.run(optimizer, feed_dict={x_: rand_x, y_: rand_y})\n",
    "graph_loss = []\n",
    "for epochs in range(100):\n",
    "    sess.run(optimizer, feed_dict={x_: rand_x, y_: rand_y})\n",
    "    epoch_loss = sess.run(loss,feed_dict={x_: rand_x, y_: rand_y} )\n",
    "    graph_loss.append(epoch_loss)\n",
    "    print(\"epochs:\",epochs,\" epoch_loss:\",epoch_loss,\" m:\",sess.run(m),\" b:\",sess.run(b))\n",
    "x=[]\n",
    "x.extend(range(100))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,graph_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<h4>Results Discussion</h4>\n",
    "<p></p>\n",
    "The results for fixed learning rate of .1 are here:\n",
    "    <img src=\"fixlearn.png\">\n",
    "<p></p>\n",
    "The results for fixed learning rate decay starting at .1 are here:\n",
    "    <img src=\"decaylearn.png\">\n",
    "\n",
    "<p></p>\n",
    "Is the lower loss for the fixed learning rate mean it is a better solution? \n",
    "<p></p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic Regression for classification. Simplest case for 2 classes. \n",
    "\n",
    "\n",
    "\n",
    "Logistic Neuron\n",
    "$y = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "\n",
    "$\\frac{dy}{dz}=y(1-y)$\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 5.1  3.5]\n",
      " [ 4.9  3. ]\n",
      " [ 4.7  3.2]\n",
      " [ 4.6  3.1]\n",
      " [ 5.   3.6]\n",
      " [ 5.4  3.9]\n",
      " [ 4.6  3.4]\n",
      " [ 5.   3.4]\n",
      " [ 4.4  2.9]\n",
      " [ 4.9  3.1]\n",
      " [ 5.4  3.7]\n",
      " [ 4.8  3.4]\n",
      " [ 4.8  3. ]\n",
      " [ 4.3  3. ]\n",
      " [ 5.8  4. ]\n",
      " [ 5.7  4.4]\n",
      " [ 5.4  3.9]\n",
      " [ 5.1  3.5]\n",
      " [ 5.7  3.8]\n",
      " [ 5.1  3.8]\n",
      " [ 5.4  3.4]\n",
      " [ 5.1  3.7]\n",
      " [ 4.6  3.6]\n",
      " [ 5.1  3.3]\n",
      " [ 4.8  3.4]\n",
      " [ 5.   3. ]\n",
      " [ 5.   3.4]\n",
      " [ 5.2  3.5]\n",
      " [ 5.2  3.4]\n",
      " [ 4.7  3.2]\n",
      " [ 4.8  3.1]\n",
      " [ 5.4  3.4]\n",
      " [ 5.2  4.1]\n",
      " [ 5.5  4.2]\n",
      " [ 4.9  3.1]\n",
      " [ 5.   3.2]\n",
      " [ 5.5  3.5]\n",
      " [ 4.9  3.1]\n",
      " [ 4.4  3. ]\n",
      " [ 5.1  3.4]\n",
      " [ 5.   3.5]\n",
      " [ 4.5  2.3]\n",
      " [ 4.4  3.2]\n",
      " [ 5.   3.5]\n",
      " [ 5.1  3.8]\n",
      " [ 4.8  3. ]\n",
      " [ 5.1  3.8]\n",
      " [ 4.6  3.2]\n",
      " [ 5.3  3.7]\n",
      " [ 5.   3.3]\n",
      " [ 7.   3.2]\n",
      " [ 6.4  3.2]\n",
      " [ 6.9  3.1]\n",
      " [ 5.5  2.3]\n",
      " [ 6.5  2.8]\n",
      " [ 5.7  2.8]\n",
      " [ 6.3  3.3]\n",
      " [ 4.9  2.4]\n",
      " [ 6.6  2.9]\n",
      " [ 5.2  2.7]\n",
      " [ 5.   2. ]\n",
      " [ 5.9  3. ]\n",
      " [ 6.   2.2]\n",
      " [ 6.1  2.9]\n",
      " [ 5.6  2.9]\n",
      " [ 6.7  3.1]\n",
      " [ 5.6  3. ]\n",
      " [ 5.8  2.7]\n",
      " [ 6.2  2.2]\n",
      " [ 5.6  2.5]\n",
      " [ 5.9  3.2]\n",
      " [ 6.1  2.8]\n",
      " [ 6.3  2.5]\n",
      " [ 6.1  2.8]\n",
      " [ 6.4  2.9]\n",
      " [ 6.6  3. ]\n",
      " [ 6.8  2.8]\n",
      " [ 6.7  3. ]\n",
      " [ 6.   2.9]\n",
      " [ 5.7  2.6]\n",
      " [ 5.5  2.4]\n",
      " [ 5.5  2.4]\n",
      " [ 5.8  2.7]\n",
      " [ 6.   2.7]\n",
      " [ 5.4  3. ]\n",
      " [ 6.   3.4]\n",
      " [ 6.7  3.1]\n",
      " [ 6.3  2.3]\n",
      " [ 5.6  3. ]\n",
      " [ 5.5  2.5]\n",
      " [ 5.5  2.6]\n",
      " [ 6.1  3. ]\n",
      " [ 5.8  2.6]\n",
      " [ 5.   2.3]\n",
      " [ 5.6  2.7]\n",
      " [ 5.7  3. ]\n",
      " [ 5.7  2.9]\n",
      " [ 6.2  2.9]\n",
      " [ 5.1  2.5]\n",
      " [ 5.7  2.8]\n",
      " [ 6.3  3.3]\n",
      " [ 5.8  2.7]\n",
      " [ 7.1  3. ]\n",
      " [ 6.3  2.9]\n",
      " [ 6.5  3. ]\n",
      " [ 7.6  3. ]\n",
      " [ 4.9  2.5]\n",
      " [ 7.3  2.9]\n",
      " [ 6.7  2.5]\n",
      " [ 7.2  3.6]\n",
      " [ 6.5  3.2]\n",
      " [ 6.4  2.7]\n",
      " [ 6.8  3. ]\n",
      " [ 5.7  2.5]\n",
      " [ 5.8  2.8]\n",
      " [ 6.4  3.2]\n",
      " [ 6.5  3. ]\n",
      " [ 7.7  3.8]\n",
      " [ 7.7  2.6]\n",
      " [ 6.   2.2]\n",
      " [ 6.9  3.2]\n",
      " [ 5.6  2.8]\n",
      " [ 7.7  2.8]\n",
      " [ 6.3  2.7]\n",
      " [ 6.7  3.3]\n",
      " [ 7.2  3.2]\n",
      " [ 6.2  2.8]\n",
      " [ 6.1  3. ]\n",
      " [ 6.4  2.8]\n",
      " [ 7.2  3. ]\n",
      " [ 7.4  2.8]\n",
      " [ 7.9  3.8]\n",
      " [ 6.4  2.8]\n",
      " [ 6.3  2.8]\n",
      " [ 6.1  2.6]\n",
      " [ 7.7  3. ]\n",
      " [ 6.3  3.4]\n",
      " [ 6.4  3.1]\n",
      " [ 6.   3. ]\n",
      " [ 6.9  3.1]\n",
      " [ 6.7  3.1]\n",
      " [ 6.9  3.1]\n",
      " [ 5.8  2.7]\n",
      " [ 6.8  3.2]\n",
      " [ 6.7  3.3]\n",
      " [ 6.7  3. ]\n",
      " [ 6.3  2.5]\n",
      " [ 6.5  3. ]\n",
      " [ 6.2  3.4]\n",
      " [ 5.9  3. ]]\n",
      "Y: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "print(\"X:\",X)\n",
    "print(\"Y:\",Y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Softmax regression model/classification and cross entropy</h4>\n",
    "\n",
    "\n",
    "For more than 2 classes we can use a combination of logistic regression models or softmax regression which is the\n",
    "generalization of logistic regression to multiple classes. \n",
    "\n",
    "\n",
    "We saw the logistic function used to output probability and \n",
    "$\\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$ from a dataset with input X, class Y per input and with m trained \n",
    "samples. Using Ng's notation our hypothesis/logistic is: \n",
    "$\n",
    "\\begin{align}\n",
    "h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)},\n",
    "\\end{align}\n",
    "$\n",
    "where $\\theta$ is our weight matrix. Hinton uses notation $y=\\frac{1}{1+exp(-z)}$\n",
    "\n",
    "For multiple classes logistc loss also called the cross entropy loss function is: \n",
    "    \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} \\log h_\\theta(x^{(i)}) + (1-y^{(i)}) \\log (1-h_\\theta(x^{(i)})) \\right]\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "which is derived from the law of total probability for multiple classes. Ecah class is reprented by $y_k$ where for 10 classes k would be 1..10\n",
    "\n",
    "The softmax loss is: \n",
    "    \n",
    "$p(y^{(i)} = j | x^{(i)} ; \\theta) = \\frac{e^{\\theta_j^T x^{(i)}}}{\\sum_{l=1}^k e^{ \\theta_l^T x^{(i)}} }$\n",
    "\n",
    "and the derivative of the logistic loss is in Ng notation:\n",
    "    \n",
    "$    \n",
    "\\begin{align}\n",
    "\\nabla_{\\theta_j} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m}{ \\left[ x^{(i)} \\left( 1\\{ y^{(i)} = j\\}  - p(y^{(i)} = j | x^{(i)}; \\theta) \\right) \\right]  }\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Hinton calls Cross entropy as the right cost function to use for softmax. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RELU activation function\n",
    "\n",
    "$ f(x)=max(0,x)$\n",
    "\n",
    "The RELUa activation function is used in multilayer deep networks, especially CNNs to compensate for vanishing gradients \n",
    "during backpropagation. The last layer is a FC layer to turn the output into a probability distribution. One of the problems\n",
    "with a RELU is returning a 0 causing a multiply by 0 which destroys coefficients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    if x<=0:\n",
    "        return 0\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELU activation function\n",
    "\n",
    "$\n",
    "  selu(x)=\\lambda \\begin{cases}\n",
    "               x \\;for \\;x>0\\\\\n",
    "               \\alpha e^x-a \\;for \\;x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "  \\frac {\\partial selu(x)}{\\partial x}=\\lambda \\begin{cases}\n",
    "               1 \\;for \\;x>0 \\\\\n",
    "               \\alpha e^x \\;for\\; x<0\\\\\n",
    "            \\end{cases}\n",
    "$\n",
    "\n",
    "with Î»=1.0507,a=1.6733\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(lambda,x):\n",
    "    \"\"\"if x>0 1, else 0.\"\"\"\n",
    "    return np.maximum(0,x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
